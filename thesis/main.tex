\chapter{Generative Diffeomorphic Deformation Models}
Generative models have been successfully applied in a wide range of medical image analysis tasks such as TODO list some applications. 

Diffeomorphic deformations of particular interest due to the organic nature and modeling of gradual changes
Contrasts with additive changes in e.g. classic CNN

Diffeomorphic transformations are differentiable and invertible and therefore topology preserving

diffeomorphism important because of understanding/grasp of processes


In this section, we discuss the general architecture of our model. We first examine the brain registration model proposed in \cite{voxelmorph} and then discuss our adaptation for the generative brain aging task.

\section{Voxelmorph}
\label{chap:voxelmorph}
In medical imaging, deformable image registration tackles the problem of warping one image onto another. More formally, given two scans $x$ and $y$, the aim is to find a deformation function $\Phi$ such that $x \circ \Phi$ is similar to $y$.
Dalca et al propose a deep learning architecture to learn such a mapping for 3-dimensional MRI brain data. Specifically, given $x$ and $y$ the model generates a stationary velocity field $v$ which defines the deformation ${\Phi : \R^3 \rightarrow \R^3}$ mapping $x$ to $y$ through the ordinary differential equation (ODE)
\begin{equation} \label{eq:voxODE}
	\frac{\partial \Phi^{(t)}}{\partial t} = v(\Phi^{(t)})
\end{equation}

where $\Phi^{(0)} = id$ is the identity transformation and t is time.
The final deformation field $\Phi^{(1)}$ is then obtained by integrating the field $v$ over time $t = [0, 1]$, which is computed numerically using the scaling and squaring method.

In group theory, $v$ is a member of the Lie algebra and is exponentiated to produce $\Phi^{(1)} = \exp(v)$.
The collection $\{\Phi^{(t)}\}_{t \; \in \; [0,1]}$ forms a one-parameter subgroup of diffeomorphisms and therefore for any scalars $t$ and $t'$ we have 
\begin{equation} \label{eq:voxoneparamsubgroup}
	\exp((t + t')v) = \exp(tv) \circ \exp(t'v)
\end{equation}

where $\circ$ is a composition map associated with the Lie group. Consequently, we can then use the recurrence
\begin{equation} \label{eq:voxrecurrence}
	\Phi^{(1/2^{(t-1)})} = \Phi^{(1/2^{t})} \circ \Phi^{(1/2^{t})}
\end{equation}

starting from $\Phi^{(1/2^T)}$ to obtain $\Phi^{(1)} = \Phi^{(1/2)} \circ \Phi^{(1/2)}$ where $T$ is chosen such that $v \approx 0$.

The model uses a variational inference method to generate a stationary displacement field $z$ which defines the deformation $\Phi_z$ through the ODE (\ref{eq:voxODE}). The prior probabilty of $z$ is modeled as
\begin{equation}
	p(z) = \mathcal{N}(z; 0, \Sigma_z)
\end{equation}

Spatial smoothness of z is is encouraged by letting ${\Sigma_z^{-1} = \Lambda_z = \lambda L}$ where $\Lambda_z$ is a precision matrix, $L$ is the Laplacian of a neighborhood graph defined as $L = D - A$, with graph degree matrix $D$ and voxel adjacency matrix $A$, and $\lambda$ denotes a parameter controlling the scale of the velocity field.

The target image $y$ is interpreted as a noisy observation of the warped image~$x$
\begin{equation}
	p(y|z;x) = \mathcal{N}(y; x \circ \Phi_z, \sigma^2 \mathbbm{I})
\end{equation}

with $\sigma^2$ reflecting the variance of the additive noise.

A likely registration field $\Phi_z$ can then obtained by sampling $z$ from the posterior distribution $p(z | x; y)$.
However, computing this distribution is intractable in this setting and hence a variational approach is used where $z$ is sampled from an approximate posterior probability $q_\psi(z | x; y)$ parametrized by $\psi$. The distribution is modeled as a multivariate normal
\begin{equation}
	q_\psi(z | x; y) = \mathcal{N}(z; \mu_{z | x, y}, \Sigma_{z | x, y})
\end{equation}

and approximated by minimizing the KL divergence
\begin{equation}
	\begin{split}
		  &\min_\psi KL [ q_\psi(z | x; y) || p(z | x; y) ] \\
		= &\min_\psi KL [ q_\psi(z | x; y) || p(z) ] - \E_q [ \log p(y | z; x) ]
	\end{split}
\end{equation}

The complete loss function can be separated into three terms denoted as follows
\begin{equation} \label{eq:voxloss}
	\begin{split}
		\mathcal{L}(\psi; \mathbf{x}, \mathbf{y})
		& = -\E_{q}[ \log p( \mathbf{x} | \mathbf{z}; \mathbf{y} ) ]
		+ \text{KL} [ q_{\psi} ( \mathbf{z} | \mathbf{x} ; \mathbf{y} ) || p ( \mathbf{z} ) ] \\[12pt]
		& = \underbrace{
			\frac{1}{2 \sigma^2} \norm{\mathbf{y} - \mathbf{x} \circ \Phi_{z}}^{2} \vphantom{\frac{1}{2_{2_2}}}
		}_{\text{reconstruction term}} \\[6pt]
		& + \frac{1}{2} \bigg[
		\underbrace{
			tr( \lambda \mathbf{D} \Sigma_{z | x; y} - \log \abs{ \Sigma_{z | x; y} } ) \vphantom{\mu_{z | x; y}^{T}}
		}_{\text{sigma term}} +
		\underbrace{
			\mu_{z | x; y}^{T} \Lambda_{z} \mu_{z | x; y}
		}_{\text{precision term}} \bigg]
	\end{split}
\end{equation}

The first term enforces similarity between the target image $y$ and the warped source image $x \circ \Phi_z$, the second term encourages the posterior to be close to the prior $p(z)$ while the third term spatially smoothes the mean $\mu_{z | x, y}$.

The paramaters $\mu_{z | x, y}$ and $\Sigma_{z | x, y}$ are estimated by a convolutional neural network (CNN). The architecture, which takes $x$ and $y$ as input, is based on a fully convolutional 3D UNet consisting of a convolutional layer of 16 filters followed by four downsampling layers with strides of two and three upsampling layers of 32 filters each. All convolutional layers use leaky ReLU activations and kernels of size $3\times3\times3$. TODO illustration of network (don't use from paper, need to do it for my own archs anyway)

The subsequent layer then samples a new stationary velocity field $z_k \sim \mathcal{N}(\mu_{z | x, y}, \Sigma_{z | x, y})$ using the reparameterization trick \cite{kingma2013}, followed by newly introduced scaling and squaring layers to compute $\Phi_{z_k} = \exp(z_k)$. Specifically, one such layer performs a differentiable vector field composition, that is, given vector fields $a$ and $b$, it computes $(a \circ b)(p) = a(b(p))$ for each voxel $p$. Note that linear interpolation is used in $a$ as $b(p)$ generally yields a non-integer location. The recurrence in \autoref{eq:voxrecurrence} is implemented using $T = 7$ of these layers. Finally, a spatial transform layer applies the deformation field $\Phi_{z_k}$ to the source image $x$ to obtain $x \circ \Phi_{z_k}$.

The network is implemented in Keras with a Tensorflow backend and trained end-to-end using the Adam \cite{Adam} optimizer.


\begin{equation}
	\mathcal{L}(\psi; x, y) = \mathcal{L}_{rec} + \mathcal{L}_{KL} + \text{const} 
\end{equation}

\begin{equation}
	\frac{\lambda}{2} \sum \sum_{j \in N(I)} ( \mu[i] - \mu[j])^{2}
\end{equation}



\begin{equation}
	\mathcal{L}_{gen} = \mathcal{L}_{D} + \mathcal{L}_{sim} + \mathcal{L}_{KL} + \mathcal{L}_{reg} + \mathcal{L}_{clf} 
\end{equation}

\begin{equation}
	\mathcal{L}_{cri} = \mathcal{L}_{D} + \mathcal{L}_{sim} + \mathcal{L}_{KL} + \mathcal{L}_{reg} + \mathcal{L}_{clf} 
\end{equation}



\section{Adaptation for Brain Aging}
While the tasks of brain registration and generative brain aging may not appear to have much in common at first, both can be described in terms of learning a deformation function. As such, the approach used in \cite{voxelmorph} can presumably be adapted to suit brain aging as well. However, while there are simililarites, a number of key differences in the problem settings require modifications to the model design.

Most importantly, the brain registration task as defined in \cite{voxelmorph} and described above is an unsupervised learning problem where both the source image $x$ and the target image $y$ are available in the prediction step. Conversely, since the goal of the brain aging task is to predict the future state of $x$, the aged target image $y$ is only available in training and therefore cannot be a part of the model's input.

Furthermore, the learned deformations for the brain aging task can be expected to be much smaller in scale and therefore, the noise introduced as part of the reconstruction term in \autoref{eq:voxloss} may have a negative effect on the model performance.

Finally, while intermediate deformations $\Phi^{(t)}$ for time steps $t \notin \{0, 1\}$ are not of primary interest in the brain registration task, the ability to predict a brain image $\hat y^{(t)} = x \circ \Phi_z^{(t)}$ for arbitrary $t$ promises valuable insights into the progression of neurodegenerative diseases as well as the brain's aging process in general.
Furthermore, the ability to train on image pairs with a large range of time steps $t$ is also crucial as the number of image pairs for any particular fixed $t$ is very limited. Moreover, a model trained on a range of time steps should be able to generalize much more  TODO ugly sentence, fix, also mention better generalization with different t

\subsection{Adversarial Loss}
% maybe say what we do first, then why?
%First, we replace the reconstruction term of the loss function \ref{eq:voxloss} with an adversarial loss.

As described above, the model input is restricted to the source image $x$ and, without access to $y$, predicting differences between the source $x$ and target $y$ that are not related to aging, such as artifacts introduced during scaning or preprocessing (e.g. skull remnants or misalignment), is virtually impossible. As a consequence, our loss function should be invariant to such changes, yet this is not the case for the reconstruction term. Moreover, the term introduces image noise which can be problematic given the small scale of aging related changes.

%As described above, the model input is restricted to the source image $x$ due to the supervised nature of the brain aging problem. However, without access to $y$, predicting differences between the source $x$ and target $y$ that are not related to aging, such as artifacts induced during scaning or preprocessing (e.g. skull remnants or misalignment), is virtually impossible. As a consequence, our loss function should be invariant to such changes. Moreover, the reconstruction loss term in \autoref{eq:voxloss} considers $y$ to be a noisy observation of $ x \circ \Phi_z $. While this works well for relatively large deformations, aging-related changes are much smaller in scale

Therefore, we remove the reconstruction loss term in \autoref{eq:voxloss} and replace it with an adversarial loss component. We achieve this by adding a secondary critic network to the architecture which is trained alongside the generator in an adversarial fashion. Effectively, this tranforms the model into a Generative Adversarial Network (GAN) \cite{GAN}.

In the adversarial setting, a generative model $G$ and a discriminative model $D$ are engaged in a minimax game, where .



\begin{equation}
	\mathcal{L}_{GAN}(M, D) = \E _ { x \sim p_d(x | c = 0) } [ D (x) ] 
	 - \E _ { x \sim p_d(x | c = 1) } [ D (x + M(x)) ].
\end{equation}

\begin{equation}
	\mathcal{L}_{reg} (M) = \norm{M(x)}_1.
\end{equation}

\begin{equation}
	M^* = \argmin_M \max_{D \in \mathcal{D}} \mathcal{L}_{GAN}(M, D) + \lambda \mathcal{L}_{reg} (M),
\end{equation}

where $\mathcal{D}$ is the set of 1-Lipschitz functions.

Similar to \cite{VAGAN} we additionally introduce a similarity loss $\mathcal{L}_{sim}$ defined as the $L_1$ difference between the original image $x$ and the warped image $\hat y = x \circ \Phi_z$
\begin{equation}
	\mathcal{L}_{sim}( x, \hat y ) = \norm{ \hat y - x }_1
\end{equation}

Finally, we add a loss term to encourage sparseness of the velocity fields
\begin{equation}
	\mathcal{L}_{sparse}( \mu_z ) = \norm{\mu_z}_1
\end{equation}


\subsection{Arbitrary Time Step Training and Prediction}
The scaling and squaring method as described in \autoref{chap:voxelmorph} is fixed to one specific time step $t$ determined by the model configuration as well as the training data. As mentioned above, this is not necessarily an issue in the case of image registration but highly desirable for the brain aging task. Therefore, one of our goals is to enable training and prediction on arbitrary time steps.

One straightforward approach is to abandon the scaling and squaring method in favor of iterative composition
\begin{equation}
	\Phi^{(t)} =
	\underbrace{
		\Phi^{(1 / 2^T)} \, \circ \, \ldots \, \circ \, \Phi^{(1 / 2^T)} \vphantom{\Phi^{(1 / 2^T)}_2}
	}_{\lceil 2^T \times \: t \rceil \ \text{times}}
\end{equation}

where $2^T$ is the scaling factor and $t$ is the desired time step. Given a large enough $T$, this method can handle any positive time step with arbitrary precision, however very quickly at the cost of computional unfeasibility. Similarly, we could use a two step approach, calculating the deformation $\Phi^{(\epsilon)}$ for some time step $\epsilon$ by scaling and squaring, followed by iterative composition of $\Phi^{(\epsilon)}$. While this is much faster in practice, the choice of $\epsilon$ represents a trade-off between precision, data availability and computational viability.

In addition to the final deformation field $\Phi^{(1)}$, the recurrence also yields intermediate deformations $\{ \Phi^{(1 / 2^{t})} \}_{t \; \in \; 1 .. T} $ at no additional computational cost. For instance, the computation of a deformation field corresponding to a time step of 8 years additionally yields the deformations for (and therefore the ability to predict and train on) time steps of ${ 4, 2, 1, 0.5, \ldots }$ years. While this represents an improvement, the benefits are relatively minor as we are still limited to a small and very specific set of time steps.

However, from the properties of one-parameter subgroups in \autoref{eq:voxoneparamsubgroup} we know that any two given deformations $\Phi^{(t)}$ and $\Phi^{(t')}$ can be composed to obtain ${ \Phi^{(t + t')} = \Phi^{(t)} \circ \Phi^{(t')} }$. It follows that for any time step $t \in [0, 1)$, the corresponding deformation $\Phi^{(t)}$ can be represented as a composition of deformations from a subset $\mathcal{S}^{(t)} \subset \{ \Phi^{(1 / 2^{t})} \}_{t \; \in \; 1 .. T} $ of intermediate deformations
\begin{equation}
	\Phi^{(t)} = \underset{\Phi^{(i)} \; \in \; \mathcal{S}^{(t)}}{\bigcirc} \Phi^{(i)}
\end{equation}

In other words, $\{ \Phi^{(1 / 2^{s})} \}_{s \; \in \; 1 .. T}$ can be interpreted as a set of vectors that span the space of all deformations $\Phi^{(t)}$ for $t \in [0, 1)$, where each $\Phi^{(t)}$ is uniquely represented by a binary vector in this space. An example of one such composition is shown in TODO figure ref.

\input{tikz/timestep}

\chapter{Applications}

want to predict brain after given time step
useful for diagnostics (use existing methods)
useful for understanding (different brains, same time step)

\section{Long Term Prediction}
not realistic but works to see trends

\section{Conversion Prediction}
Want to know whether patients with Mild Cognitive Impairment convert to AD patients

\section{Feature Attribution}
probabilistic model (sampling from distribution)
predict multiple images instead of just one
get heat map of changes

\chapter{Data}

\section{Synthetic Data}
In order to validate our architecture, we first train and evaluate our model on a synthetic data set designed to yield easily interpretable results while still being similar in structure to the preprocessed brain data. TODO ref to figure with toy data

Each sample consists of a pair $(x_i, y_i)$ of $ 80 \times 96 \times 80 $ images, containing a spherical shell with a value of $-1$ on its shell and $1$ in its interior. We randomize both the sphere's radius and position within the image, and sample $t_i \sim \mathcal{U}(0, 1)$, the time step between $x_i$ and $y_i$. The shell's thickness decreases from $x_i$ to $y_i$, where the thickness in $y_i$ is defined as $d_{y_i} = (1-t) d_x$, with $d_x$ identical for all $x_i$. We explore two different backgrounds, a constant value of 0 as well as smoothed gaussian noise identical for $x_i$ and $y_i$. TODO maybe more complex toy data

We generate a total of 20'000 samples, using 60\% of the data set for training and 20\% for evaluation and testing each.

\section{MRI Data}
For the 

\subsection{Data Sources}
We use a large data set of 19'000 TODO(exact numbers) brain MRI scans from the publicly available Alzheimer's Disease Neuroimaging Initiative (ADNI) \cite{ADNI} and Australian Imaging Biomarkers and Lifestyle (AIBL) \cite{AIBL} studies.

\subsection{Data Processing}
Our data processing pipeline consists of three primary steps:

\begin{itemize}
\item Extraction
\item Registration
\item Segmentation
\end{itemize}

In the first step known as brain extraction or alternatively skull stripping, we extract the brain from the surrounding non-brain tissue and then secondly align the resulting images to a common reference atlas using linear transformations with 12 degrees of freedom. Both steps are performed using the FSL toolkit \cite{FSL}, using the \texttt{bet} and \texttt{flirt} commands respectively. Thirdly, we segment each voxel into one of three classes, White Matter (WM), Gray Matter (GM) and Cerebrospinal Fluid (CSF) while simultaneously correcting a scanner-related image artifact known as the bias field using FSL's \texttt{fast} command. The results of this operation are three voxel-wise probability maps for the different classes and we then proceed to subtract the WM map from the GM map while dropping the CSM map. This results in a new image with a number of potentially benefitial properties, where all voxel values are restricted to the range [-1, 1] and can be directly compared across different images. Note that the MR imaging process captures relative intensity differences and as a consequence, direct comparison of absolute values is in general not possible for raw or even unit gaussian normalized data. Furthermore, the operation enhances the structural contrast and removes low level variance in the image. We choose this apprach based on the assumption, that most of the information relevant to the brain aging process is contained in the structural changes of the segmentation (TODO experiment on data), with smaller differences in intensity most likely representing noise.
Note that since the output of our preprocessing pipeline is a segmentation mask, one could combine the T1-weighted scans with data from different brain imaging modalities such as T2-weighted MRI data or proton density (PD) scans, therefore drastically increasing the number of possible data sources. However, we do not validate or pursue this idea in the context of this thesis.

\subsection{Data Splitting}

different splits for different tasks
dx classifier
conversion set
leave one visit out
singles
pairs

\chapter{Experiments}

\section{Synthetic Data}
results without age reg
results with age reg? somewhat tricky

\section{Voxelmorph}


\section{Diagnosis Classifier}

\section{Age Regressor}

Validating generative outputs is hard
we use an age regressor to predict the age of a given scan
use regressor on generated image
show regressor accuracy with normal eval set
explain why this is only partially relevant
show regressor performance on LOO eval set
show that delta loss is significantly better than absolute L1 loss

\section{Fixed Delta Prediction}
predict brain after a given time step
check resulting ages from age reg
plots plots plots

\section{Long Term Prediction}
train on AD only, HC only?

\section{Conversion Prediction}
F1 score, accuracy

\chapter{Related Work}

\chapter{Discussion}
