\chapter{Generative Diffeomorphic Deformation Models}
Generative models have been successfully applied to a wide range of medical image analysis tasks such as image registration \cite{balakrishnan2019voxelmorph}, segmentation \cite{dong2017automatic} and visual feature attribution \cite{baumgartner2018visual}. Of particular interest are deformation-based models due to their ability to closely model the gradual changes observed in the context of medical imaging.
Additionally, by using \textit{diffeomorphic} deformations, a model can be limited to operations which are smooth, differentiable and invertible and, as a consequence, topology preserving.
%As a consequence, diffeomorphic models are topology preserving and restricted the
Moreover, unlike convolutional models which generally implement a single atomic transformation, diffeomorphic deformations can be interpolated at any intermediate time step, therefore generally resulting in more interpretable outputs.

%This ability can be strengthened further by limiting the model to smooth and invertible operations such as diffeomorphisms, which has the added benefit of generally resulting in more interpretable outputs as well.
%To that end, deformations can be described by diffeomorphisms, that is, functions which are differentiable and invertible, and therefore topology preserving.

%In contrast to the purely convolution approach, which is in essentially a universal function approximator, deformation-based models are constrained to operations that closely match the actual processes observed in human tissue.
%Particularly in the case of MRI data, voxel intensities are relative, deformation better than additional map

In this section, we discuss the general architecture of our model. We present a brief review of the diffeomorphic brain registration model proposed in \cite{balakrishnan2019voxelmorph} \cite{dalca2018unsupervised} followed by a discussion of our adaptations for the generative brain aging task.

\section{Diffeomorphic Image Registration}
\label{chap:voxelmorph}
In medical imaging, deformable image registration tackles the problem of warping one image onto another. More formally, given two scans $x$ and $y$, the aim is to find a deformation function $\Phi$ such that $x \circ \Phi$ is similar to $y$.

\subsection{Voxelmorph} \label{sec:vox}
Dalca et al \cite{dalca2018unsupervised} propose a deep learning architecture to learn such a mapping for 3-dimensional MRI brain data. Formally, given $x$ and $y$ the model generates a stationary velocity field $v$ which defines the deformation ${\Phi : \R^3 \rightarrow \R^3}$ mapping $x$ to $y$ through the ordinary differential equation (ODE)

\begin{equation} \label{eq:voxODE}
	\frac{\partial \Phi^{(t)}}{\partial t} = v(\Phi^{(t)})
\end{equation}

where $\Phi^{(0)} = id$ is the identity transformation and $t$ is time.
The final deformation field $\Phi^{(1)}$ is then obtained by integrating the field $v$ over time $t \in [0, 1]$, which is computed numerically using the scaling and squaring method \cite{arsigny2006log}.

In group theory, $v$ is a member of the Lie algebra and is exponentiated to produce $\Phi^{(1)} = \exp(v)$.
The collection $\{\Phi^{(t)}\}_{t \; \in \; [0,1]}$ forms a one-parameter subgroup of diffeomorphisms and therefore for any scalars $t$ and $t'$ we have 
\begin{equation} \label{eq:voxoneparamsubgroup}
	\exp((t + t')v) = \exp(tv) \circ \exp(t'v)
\end{equation}

where $\circ$ is a composition map associated with the Lie group. Consequently, we can then use the recurrence
\begin{equation} \label{eq:voxrecurrence}
	\Phi^{(1/2^{(t-1)})} = \Phi^{(1/2^{t})} \circ \Phi^{(1/2^{t})}
\end{equation}

starting from $\Phi^{(1/2^T)}$ to obtain $\Phi^{(1)} = \Phi^{(1/2)} \circ \Phi^{(1/2)}$ where $T$ is chosen such that $v \approx 0$.

The model uses a variational inference method to generate a stationary displacement field $z$ which defines the deformation $\Phi_z$ through the ODE (\ref{eq:voxODE}). The prior probability of $z$ is modeled as
\begin{equation}
	p(z) = \mathcal{N}(z; 0, \Sigma_z)
\end{equation}

Spatial smoothness of z is is encouraged by letting ${\Sigma_z^{-1} = \Lambda_z = \lambda L}$ where $\Lambda_z$ is a precision matrix, $L$ is the Laplacian of a neighborhood graph defined as $L = D - A$, with graph degree matrix $D$ and voxel adjacency matrix $A$, and $\lambda$ denotes a parameter controlling the scale of the velocity field.

The generator's output is modeled as a gaussian distribution with $y$, the target image, as its mean and a standard deviation of $\sigma$. In other words, the warped image $x$ is interpreted as a noisy oberservation of the target image $y$
\begin{equation}
	p(y|z;x) = \mathcal{N}(y; x \circ \Phi_z, \sigma^2 \mathbbm{I})
\end{equation}

with $\sigma^2$ reflecting the variance of the additive noise.

A likely registration field $\Phi_z$ can then obtained by sampling $z$ from the posterior distribution $p(z | x; y)$.
However, computing this distribution is intractable in this setting and hence a variational approach is used where $z$ is sampled from an approximate posterior probability $q_\psi(z | x; y)$ parametrized by $\psi$. The distribution is modeled as a multivariate normal
\begin{equation}
	q_\psi(z | x; y) = \mathcal{N}(z; \mu_{z | x, y}, \Sigma_{z | x, y})
\end{equation}

and approximated by minimizing the KL divergence
\begin{equation}
	\begin{split}
		  &\min_\psi KL [ q_\psi(z | x; y) || p(z | x; y) ] \\
		= &\min_\psi KL [ q_\psi(z | x; y) || p(z) ] - \E_q [ \log p(y | z; x) ]
	\end{split}
\end{equation}

The complete loss function can be separated into two components, a reconstruction and a prior term. Furthermore, the latter can be split into a covariance and a precision term.
\begin{equation} \label{eq:voxloss}
	\begin{split}
		\mathcal{L}(\psi; x, y)
		& = -\E_{q}[ \log p( x | z; y ) ]
		+ \text{KL} [ q_{\psi} ( z | x ; y ) || p ( z ) ] \\[12pt]
		& = \underbrace{
			\frac{1}{2 \sigma^2} \norm{y - x \circ \Phi_{z}}^{2} \vphantom{\frac{1}{2_{2_2}}}
		}_{\text{reconstruction term}} \\[6pt]
		& + \frac{1}{2} \bigg[
		\underbrace{
			tr( \lambda D \Sigma_{z | x; y} - \log \abs{ \Sigma_{z | x; y} } ) \vphantom{\mu_{z | x; y}^{T}}
		}_{\text{covariance term}} +
		\underbrace{
			\mu_{z | x; y}^{T} \Lambda_{z} \mu_{z | x; y}
		}_{\text{precision term}} \bigg]
	\end{split}
\end{equation}

The first term enforces similarity between the target image $y$ and the warped source image $x \circ \Phi_z$, the second term encourages the posterior to be close to the prior $p(z)$ while the third term spatially smoothes the mean $\mu_{z | x, y}$. This effect can be shown more explicitly by rewriting the precision term as $ { \frac{\lambda}{2} \sum \sum_{j \in N(I)} ( \mu[i] - \mu[j])^{2} } $, where $N(i)$ denotes the set of neighbors of voxel $i$. Both $\sigma$ and $\lambda$ are treated as hyperparameters, respectively controlling the reconstruction penalty and the magnitude of the velocity field.

\subsubsection*{Network Architecture}
The parameters $\mu_{z | x, y}$ and $\Sigma_{z | x, y}$ are estimated by a convolutional neural network (CNN). The architecture, which takes $x$ and $y$ as input, is based on a fully convolutional 3D UNet consisting of a convolutional layer of 16 filters followed by four downsampling layers with strides of two and three upsampling layers of 32 filters each. All convolutional layers use leaky ReLU activations with $\alpha = 0.2$ and kernels of size $3\times3\times3$. See \autoref{fig:archvox} for an illustration of the generator model.

\begin{figure}
	\centering
	\input{tikz/arch_vox}
	\caption{Voxelmorph for diffeomorphic image registration as proposed in \cite{dalca2018unsupervised}. The UNet-based encoder receives both $x$ and $y$ as inputs and approximates the distributon $q_\psi$ from which the velocity field $z$ is sampled. Subsequently, $z$ is integrated using a configurable number of scaling and squaring layers resulting in the final deformation field $\Phi^{(1)}$ which is then applied to the input image $x$.}
	\label{fig:archvox}
\end{figure}

%Given $\mu_{z | x, y}$ and $\Sigma_{z | x, y}$, 
Using the reparameterization trick \cite{kingma2013auto}, the subsequent layer then samples a new stationary velocity field $ { z_k \sim \mathcal{N}(\mu_{z | x, y}, \Sigma_{z | x, y}) } $ , which is then integrated using scaling and squaring layers newly introduced in \cite{dalca2018unsupervised} to compute $\Phi_{z_k} = \exp(z_k)$. Specifically, one such layer performs a differentiable vector field composition, that is, given vector fields $a$ and $b$, it computes $(a \circ b)(p) = a(b(p))$ for each voxel $p$. Note that linear interpolation is used in $a$ as $b(p)$ generally yields a non-integer location. The recurrence in \autoref{eq:voxrecurrence} is implemented using $T = 7$ of these layers. Finally, a spatial transform layer applies the deformation field $\Phi_{z_k}$ to the source image $x$ to obtain $x \circ \Phi_{z_k}$.

The network is implemented in Keras with a Tensorflow backend and trained end-to-end using the Adam \cite{kingma2014adam} optimizer.

\section{Adaptation for Brain Aging}
\label{sec:ada}
While the tasks of brain registration and generative brain aging may not appear to have much in common on the surface, both can be described in terms of learning a deformation function. As such, the approach used in \cite{balakrishnan2019voxelmorph} can be adapted for the brain aging setting. However, while there are simililarites, a number of key differences in the problem settings require modifications to the model design.

Most importantly, the brain registration task as defined in \cite{balakrishnan2019voxelmorph} and described above is an unsupervised learning problem where both the source image $x$ and the target image $y$ are available in the prediction step. Conversely, since the goal of the brain aging task is to predict the future state of $x$, the aged target image $y$ is only available during training and therefore cannot be a part of the model's input.

Furthermore, the learned deformations for the brain aging task can be expected to be much smaller in scale, therefore increasing the relative magnitude of the noise introduced as part of the reconstruction term in \autoref{eq:voxloss}. In turn, this lowers the model's ability to capture subtle changes which may negatively affect its performance in the brain aging setting. While this problem can be addressed by lowering the hyperparameter $\sigma$, this comes at the cost of decreased generalization as the model is forced to produce results which are progressively closer to the target image $y$.

Finally, while intermediate deformations $\Phi^{(t)}$ for time steps $t \notin \{0, 1\}$ are not of primary interest in the brain registration task, the ability to predict a brain image $G(x) = x \circ \Phi_z^{(t)}$ for arbitrary $t$ promises valuable insights into the progression of neurodegenerative diseases as well as the brain's aging process in general.
Furthermore, the ability to train on image pairs over a large range of different time steps is crucial as the number of image pairs for any particular fixed $t$ is very limited. Moreover, training on a continuous range of time steps as opposed to a limited number of fixed intervals should result in improved generalization.

\subsection{Adversarial Loss}
% maybe say what we do first, then why?
%First, we replace the reconstruction term of the loss function \ref{eq:voxloss} with an adversarial loss.

As described above, the model input is restricted to the source image $x$ and, without access to $y$, predicting differences between the source $x$ and target $y$ that are not related to aging, such as artifacts introduced during scanning or preprocessing (e.g. skull remnants or misalignment), is virtually impossible. As a consequence, our loss function should be invariant to such changes, yet this is not the case for the reconstruction term. Moreover, the term introduces image noise which can be problematic given the small scale of aging related changes.

%As described above, the model input is restricted to the source image $x$ due to the supervised nature of the brain aging problem. However, without access to $y$, predicting differences between the source $x$ and target $y$ that are not related to aging, such as artifacts induced during scaning or preprocessing (e.g. skull remnants or misalignment), is virtually impossible. As a consequence, our loss function should be invariant to such changes. Moreover, the reconstruction loss term in \autoref{eq:voxloss} considers $y$ to be a noisy observation of $ x \circ \Phi_z $. While this works well for relatively large deformations, aging-related changes are much smaller in scale

Therefore, we opt to replace the reconstruction loss term in \autoref{eq:voxloss} with an adversarial loss component. We realize this by adding a secondary critic network to the architecture which is trained alongside the generator in an adversarial fashion. Effectively, this tranforms the model into a Generative Adversarial Network (GAN) \cite{goodfellow2014generative}.

In the adversarial setting, a generative model $G$ and a discriminative model $D$ are engaged in a minimax game, in which the generator aims to produce outputs that to the discriminator are indistinguishable from samples drawn from a real data distribution $p_{data}$. More formally, a GAN optimizes the objective
\begin{equation}
	\min_G \max_D V(G, D) = \E_{ x \sim p_{data}(x) } [ \: \log{D (x)} \: ] 
	 - \E_{ z \sim p_z(z) } [ \: 1 - \log{D (G(z))}) \: ]
\end{equation}

where $D(x)$ is a probability and $z$ is usually sampled from a latent distribution. However, in the brain aging setting the goal is to transform a source image $x$ in a way that resembles the actual aging process and therefore we get the revised objective
\begin{equation}
	\begin{split}
		\min_G \max_D V(G, D) = \; & \E_{ (x, y, t) \sim p_{data} } [ \: \log{D (x, y, t)}] \: ] \\
		 - & \E_{ (x, t) \sim p_{data} } [ \: 1 - \log{D (x, G(x), t)}) \: ]
	\end{split}
\end{equation}

where the image pair $(x, y)$ and the corresponding age difference $t$ are samples from the real data distribution. Note that in order to avoid the issue of mode collapse, where the generator outputs the same image for all inputs, the discriminator also observes $x$. Furthermore, to enable the discriminator to discern pairs with differing time steps, we additionally pass $t$ as an input. Both $G$ and $D$ are implemented as neural networks which are trained in an alternating fashion. 

We use a variation of the original GAN known as Wasserstein GAN (WGAN) \cite{arjovsky2017wasserstein} in which the discriminator $D$ is replaced by a critic with real-valued outputs instead of probabilities. The critic is limited to the set of 1-Lipschitz functions, which is enforced by imposing a gradient penalty as proposed in \cite{gulrajani2017improved}.

\subsection{Arbitrary Time Step Scaling and Squaring}
\label{sec:adaarbtimestep}
The scaling and squaring method as described in \autoref{chap:voxelmorph} is fixed to one specific time step $t$ determined by the model configuration as well as the training data. As mentioned above, this is not necessarily an issue in the case of image registration but highly undesirable for the brain aging task. Therefore, in this section we propose an extension to the scaling and squaring method enabling integration of the stationary velocity field $v$ to arbitrary time steps $t$. As a result of this modification, our model can predict and be trained on image pairs with arbitrary time steps greatly increasing the available training data as well as its ability to generalize over different time ranges.

One straightforward approach is to abandon the scaling and squaring method in favor of iterative composition
\begin{equation}
	\Phi^{(t)} =
	\underbrace{
		\Phi^{(1 / 2^T)} \, \circ \, \ldots \, \circ \, \Phi^{(1 / 2^T)} \vphantom{\Phi^{(1 / 2^T)}_2}
	}_{\lceil 2^T \times \: t \rceil \ \text{times}}
\end{equation}

where $2^T$ is the scaling factor and $t$ is the desired time step. Given a large enough $T$, this method can handle any positive time step with arbitrary precision, however very quickly at the cost of computional infeasibility. Similarly, we could use a two step approach, calculating the deformation $\Phi^{(\epsilon)}$ for some time step $\epsilon$ by scaling and squaring, followed by iterative composition of $\Phi^{(\epsilon)}$. While this is much faster in practice, the choice of $\epsilon$ represents a trade-off between precision, data availability and computational viability.

We observe that in addition to the final deformation field $\Phi^{(1)}$, the recurrence also yields intermediate deformations $\{ \Phi^{(1 / 2^{t})} \}_{t \; \in \; 1 .. T} $ at no additional computational cost. For instance, the computation of a deformation field corresponding to a time step of 8 years additionally yields the deformations for (and therefore the ability to predict and train on) time steps of ${ 4, 2, 1, \sfrac{1}{2}, \ldots }$ years. While this represents an improvement, the benefits are relatively minor as we are still limited to a small and very specific set of time steps.

However, from the properties of one-parameter subgroups in \autoref{eq:voxoneparamsubgroup} we know that any two given deformations $\Phi^{(t)}$ and $\Phi^{(t')}$ can be composed to obtain ${ \Phi^{(t + t')} = \Phi^{(t)} \circ \Phi^{(t')} }$. It follows that for any time step $t \in [0, 1)$, the corresponding deformation $\Phi^{(t)}$ can be approximated to within a temporal precision factor $\epsilon$ by composing deformations from a subset $\mathcal{S}^{(t)} \subset \{ \Phi^{(1 / 2^{t})} \}_{t \; \in \; 1 .. T} $ of intermediate deformations
\begin{equation}
	\Phi^{(t)} = \underset{\Phi^{(i)} \; \in \; \mathcal{S}^{(t)}}{\bigcirc} \Phi^{(i)}
\end{equation}

In other words, $\{ \Phi^{(1 / 2^{s})} \}_{s \; \in \; 1 .. T}$ can be interpreted as a set of vectors that span the space of all deformations $\Phi^{(t)}$ for $t \in [0, 1)$, where each $\Phi^{(t)}$ is uniquely represented by a binary vector in this space. Intuitively speaking, this is analogous to how any positive integer can be expressed in $base_2$ as the sum over a set of powers of 2. The deformation is computed iteratively over all squaring steps as laid out in \autoref{alg:arbtimestep}. Refer to \autoref{fig:arbtimestep} for a visual example of one such composition.

The temporal precision $\epsilon$, i.e. the smallest difference in time steps representable by the model, is determined by the number of squaring steps $T$ as well as the maximum time step $t_{max}$ used during training. Specifically, $\epsilon$ is the time step corresponding to the smallest deformation field $\Phi^{(\sfrac{1}{2^T})} = \sfrac{v}{2^T}$ and therefore $\epsilon = \sfrac{t_{max}}{2^T}$. As an example, given $t_{max} = 6$ years and $T = 7$, $\epsilon = 0.046$ years or approximately $17$ days.

In practice, the efficiency of the calulation can be improved by computing only the deformations up to the largest intermediate step required in the composition of $\Phi^{(t)}$. We also note that predictions for time steps $t > 1$ can be generated by dynamically increasing the number of squaring layers during inference.

\input{algo/timestep}

\begin{figure}
	\centering
	\input{tikz/timestep}
	\caption{Arbitrary time step scaling and squaring with $T = 3$ squaring and $ 2^{T} = 8 $ atomic steps, shown for one voxel $ \vec p $. The deformation $ \Phi^{(t)} $ can be approximated at any time step $ t \in [0, 1] $ by composing a subset of intermediate deformations. Note that in practice, larger $T$ are used resulting in an exponentially higher number of atomic steps and therefore a better approximation, e.g. $T = 7$ yielding $ 2^7 = 128 $ atomic steps. We calculate the deformation for all voxels $ \vec{p} $ in parallel.} \label{fig:arbtimestep}
\end{figure}

\subsection{Additional Loss Terms}
In addition to the adversarial loss we also examine four additional loss terms and their effects on the model performance.

\subsubsection*{Age Regressor}
\label{sec:adaagereg}
While the diffeomorphic approach encourages the model to generate realistically aged $G(x) = x \circ \Phi^{(t)}$ with respect to time step $t$, we strengthen this further by using a pre-trained age regressor $R$ to estimate the apparent age of $G(x)$. In this setting, $R$ is a model which estimates a patient's age based on an MRI scan of their brain.
Let $ a_x $ denote a patient's age at the time of taking image $x$ and $ \hat a_x = R(x)$ denote the age as estimated by the age regressor on $x$. As a side note, for the generator we generally assume $t \in [0, 1]$ normalized by $\max_{(x, y) \in \mathcal{D}_{train}} a_y - a_x $, the maximum time step occuring in the training data, and therefore $t_{(x, y)} \neq a_y - a_x$ in general.

We consider two different possible loss terms 
\begin{equation}
	\begin{split}
		(1) \quad \mathcal{L}_{age}(x, y, R) & = 
		| (a_y - a_x) - (\hat a_{G(x)} - a_x) | =
		| a_y - \hat a_{G(x)} | \\[8pt]
		(2) \quad \mathcal{L}_{age}(x, y, R) & =
		| (\hat a_y - \hat a_x) - (\hat a_{G(x)} - \hat a_x) | = 
		| \hat a_y - \hat a_{G(x)} |
	\end{split}
\end{equation}

with $(1)$ using ground truth labels whenever available and $(2)$ using the regressor throughout. We hypothesize (2) to be superior due to inaccuracies of the age regressor cancelling out. This assumption is supported by our experimental results in \autoref{sec:expreg} and consequently, we use (2) for our model.

\subsubsection*{Diagnosis Classifier}
Similar to the age regressor, we also add a loss term based on a diagnosis classifier $C$ to encourage the model to understand and distinguish between different diagnoses. Let $d_x$ denote the ground truth diagnosis label assigned to $x$ (with 0 = MCI, 1 = AD) and $\hat d_x$ denote the classifier's estimated probability of the brain in image $x$ being affected by AD. As before, we examine two possible cross entropy loss terms between ground truth labels and the estimated probabilities respectively and implement $(2)$, following the same reasoning used in selecting the age regressor loss term.

\begin{equation}
	\begin{split}
		H(p, q) & = -p \log\, q - (1 - p)\log(1 - q) \\[8pt]
		(1) \quad \mathcal{L}_{dx}(x, y, C) & = 
		H(d_y, \hat d_{G(x)}) \\[8pt]
		(2) \quad \mathcal{L}_{dx}(x, y, C) & =
		H(\hat d_y, \hat d_{G(x)}) \\[8pt]
	\end{split}
\end{equation}

In addition to its use in this loss term, the classifier also serves as our baseline for the conversion prediction experiment as described in \autoref{sec:appconvpred}.

\subsubsection*{Similarity Loss}
Similar to \cite{baumgartner2018visual} and \cite{wegmayr2019generative}, we discourage the model from introducing drastic changes between the original image $x$ and the warped image $ G(x) = x \circ \Phi^{(t)} $ by imposing an $L_1$ loss on their difference
\begin{equation}
	\mathcal{L}_{sim}( x, G ) = \norm{ x - G(x) }_1
\end{equation}

Note that we choose not to scale the similarity loss with respect to time step $t$. This is based on the observation that while the $L_1$ difference does increase for larger $t$, as depicted in \autoref{fig:l1plots}, it does so rather slowly, indicating that the majority of the difference is unrelated to aging. As a side note, the weak correlation between the $L_1$ difference and time step $t$ also indicates that the metric is not well suited to validate our the aging performance of our generative model.

\subsubsection*{Sparseness Loss}
Finally, we encourage sparseness of the velocity field by imposing an $L_1$ loss on its magnitude
\begin{equation}
	\mathcal{L}_{sparse}( x, G ) = \norm{\mu_z}_1
\end{equation}

Note that while this loss term acts as a regularizer, the primary motivation for sparseness is to improve the interpretability of the deformation field by discouraging displacements with very little or no effect at all.

\subsubsection*{Complete Objective}
To summarize, we obtain the complete objective for the generator as follows

\begin{equation}
	\mathcal{L}_G =
		\mathcal{L}_{ws} +
		\lambda_{kl} \; \mathcal{L}_{kl} +
		\lambda_{age} \; \mathcal{L}_{age} + 
		\lambda_{dx} \; \mathcal{L}_{dx} + 
		\lambda_{sim} \; \mathcal{L}_{sim} + 
		\lambda_{sparse} \; \mathcal{L}_{sparse}
\end{equation}

where $\mathcal{L}_{ws}$ is the generator's component of the Wasserstein loss function and $\mathcal{L}_{kl}$ consists of the covariance and precision terms from \autoref{eq:voxloss}. All $ \lambda $ in the objective as well as $\lambda_{prior}$ in \autoref{eq:voxloss} are treated as model hyperparameters.

\subsection{Network Architecture}
Based on architectures applied to similar problems such as \cite{wegmayr2019generative} and \cite{baumgartner2018visual}, we expect the brain aging problem to be a more difficult task compared to brain registration as implemented in Voxelmorph. Therefore, we significantly increase the complexity of the UNet model as shown in \autoref{fig:archour}.
%Furthermore, for computational reasons, we generate the velocity and intermediate deformation fields in half-size and upsample the final displacement to full-size before applying it to the input image $x$. Finally,
Moreover, we experiment with a low resolution time-invariant deformation component, extracted from the UNet, to capture differences that are independent of time step $t$ such as misalignments introduced during scanning or preprocessing. More specifically, we fork the UNet's decoder into two distinct paths and bypass the scaling and squaring layers for one of them, resulting in two deformation maps to be applied to the input image $x$. Separating the deformation into two components is desirable as it allows ignoring changes independent of time, and therefore aging, during inference on unseen data.
However, in preliminary experiments, this component appears to overpower the time-dependent deformations and as a consequence, is not included in our final model architecture.

\begin{figure}
	\centering
	\input{tikz/arch_our}
	\caption{Overview of our generator and critic network architectures. The outputs of the squaring layers are applied selectively depending on the value of $t$. In the critic, depending on the step of the GAN training procedure, input $y$ is either a real or a generated image.}
	\label{fig:archour}
\end{figure}

\section{Applications}
Our primary goal is to design a generative model $G$ capable of learning and simulating the aging process of the brain. Given an input image $x$, we can then use the trained model $G$ to generate a prediction for the future state of the brain $\hat y^{(t)} = G(x) = x \circ \Phi^{(t)}$ for any time step $t$.

As shown by \cite{wegmayr2019generative}, the ability to generate realistic predictions is beneficial in the early detection of Alzheimer's Disease onset. Generative models are of particular interest since existing diagnostic tools, such as diagnosis classifiers operating on MRI scan, can be directly applied to predictions $\hat y$ without any necessary adaptations. Furthermore, the resulting deformation fields may yield insights into the progression and specific changes of neurodegenerative diseases.

\subsection{Conversion Prediction} \label{sec:appconvpred}
Early prediction of Alzheimer's Disease onset is an important area of Alz-heimer's research, with one particular interest being the Mild Cognitive Impairment (MCI) conversion problem. Given data about a patient diagnosed with MCI at some visit $v_i$, our goal is to predict the probability of that patient's diagnosis converting to AD over a given period of time $ \Delta $. 
In this context, we distinguish between progressive cases (pMCI) for which the diagnosis converts within $\Delta$, and stable cases (sMCI) for which it does not.

More specifically, a case is considered \textit{progressive} if there exists a pair of visits $(v_a, v_b)$ with examdates $(e_a, e_b)$ with $ e_b - e_a \geq \Delta $ and diagnoses $d_a = \text{MCI} $ and $d_b = \text{AD} $. Moreover, we require that the diagnosis does not revert after $v_b$, that is $ d_i = \text{AD} $ for all visits $v_i$ with $e_i > e_b$.

Conversely, a case is considered \textit{stable} if its diagnosis does not change across the entire data set and its visits span a time period of at least $\Delta$, that is $ d_i = \text{MCI} $ for all visits $v_i$ and $ { \max_{v_i \in V(s)} e_i -  \min_{v_i \in V(s)} e_i \geq \Delta } $, where $V(s)$ is the set of visits of subject $s$.

Using our model, we can generate $\hat y^{(\Delta)} = x \circ \Phi^{(\Delta)}$ and use this prediction to estimate the probability of a conversion occuring.

\subsection{Long-Term Prediction}
\label{sec:applongterm}
Another interesting application is to generate predictions for larger time steps. While we don't expect the model's predictions to be particulary accurate in this setting, especially for time steps $t \gg 1$, i.e. time steps significantly exceeding the maximum time step occuring in training phase, long-term predictions can be helpful in highlighting areas of significant change as well as in visualizing how the aging process of a healthy brain differs from that of a brain affected by AD. Since every additional squaring layer doubles the maximum time step $t$ for which we can generate an image, our model can produce outputs for very large steps at little additional computational cost.

\subsection{Feature Attribution}
Finally, similar to \cite{baumgartner2018visual}, our generator has potential applications in the area of visual feature attribution, that is, highlighting the parts of an image which are most strongly correlated to one of its labels, e.g. the subject's diagnosis.
For instance, by training the model on different subsets of our data, such as exclusively AD or HC cases, we can model the different progressions and visualize their effects either on a single image or aggregated over subsets of our data.
Finally, since our model is probabilistic, multiple different predictions for the same input image $x$ can be generated, which helps in getting an understanding of our model's uncertainty.

\chapter{Experiments}

\section{Synthetic Data}
\label{sec:datsynth}
In order to validate our architecture, we first train and evaluate our model on a synthetic data set designed to yield easily interpretable results while still being similar in structure to the preprocessed brain data.

Each sample consists of a pair $(x_i, y_i)$ of $ 80 \times 96 \times 80 $ images, containing a spherical shell with a value of $-1$ on its shell and $1$ in its interior. We randomize both the sphere's radius and position within the image, and sample $t_i \sim \mathcal{U}(0, 1)$, the time step between $x_i$ and $y_i$. The shell's thickness decreases from $x_i$ to $y_i$, where the thickness in $y_i$ is defined as $d_{y_i} = (1-t) d_x$, with $d_x$ identical for all $x_i$. We explore two different backgrounds, a constant value of 0 as well as smoothed gaussian noise identical for $x_i$ and $y_i$ as shown in \autoref{fig:toydata}.

We generate a total of 10'000 samples, using 60\% of the data set for training and 20\% for validation and testing each.

\section{MRI Data}
To train and validate our brain aging models, we use T1-weighted 3D MRI brain scans. We obtain a large data set of raw images with corresponding subject and image meta data from publically available sources and apply a preprocessing pipeline in order to extract, align and segment the brain tissue. Finally, we generate multiple different data sets tailored to our specific experiments.

\subsection{Data Sources}
We use a data set consisting of 19'480 brain MRI scans obtained from the publicly available Alzheimer's Disease Neuroimaging Initiative (ADNI) \cite{jack2008alzheimer} and Australian Imaging Biomarkers and Lifestyle (AIBL) \cite{ellis2009australian} studies. The study data was collected over the course of a total of 9976 visits spanning a time period of 15 years and involving 2794 subjects.

The dimensions of the raw scans depend on the type and model of scanner used and therefore vary slightly, with a median of $ 240 \times 256 \times 170 $. Furthermore, depending on a subject's study group assignment, images are taken at a field strengths of 1.5T or 3T.

\subsection{Image Data Preprocessing}
\label{sec:datpreproc}
Our data processing pipeline consists of three distint steps:

\begin{itemize}
\item Registration
\item Extraction
\item Segmentation
\end{itemize}

Firstly, in the registration step we align the raw images to a common reference atlas\footnote{Reference: MNI152\_T1\_1mm} using linear transformations with 12 degrees of freedom. Secondly, we extract the brain from from the surrounding non-brain tissue in what is known as skull stripping or alternatively brain extraction. Both steps are performed utilizing the FSL toolkit \cite{jenkinson2012fsl}, using the \texttt{flirt} \cite{jenkinson2001global} \cite{jenkinson2002improved} and \texttt{bet}\footnote{Parameters: \texttt{-v -f 0.3 -g -0.1}} \cite{smith2002fast} \cite{jenkinson2005bet2} commands respectively. Thirdly, we segment each voxel into one of three classes, White Matter (WM), Gray Matter (GM) and Cerebrospinal Fluid (CSF) while simultaneously correcting a scanner-related image artifact known as the bias field using FSL's \texttt{fast}\footnote{Parameters: \texttt{-t 1 -n 3 -l 20 -I 4 -O 4 -B}} \cite{zhang2001segmentation} command. The results of this operation are three voxel-wise probability maps for the different classes and we then proceed to subtract the WM map from the GM map while dropping the CSM map. This results in a new image with a number of potentially benefitial properties, where all voxel values are restricted to the range [-1, 1] and can be directly compared across different images as shown in \cite{de2018clinically}. Note that the MR imaging process captures relative intensity differences and as a consequence, direct comparison of absolute values is in general not possible for raw or even unit gaussian normalized data. Furthermore, the operation enhances the structural contrast and removes low level variance in the image. We choose this approach based on the assumption, that most of the information relevant to the brain aging process is contained in the structural changes of the segmentation, with smaller differences in intensity most likely representing noise. \autoref{fig:preproc} shows the entire preprocessing pipeline and all its intermediate steps applied to one sample from our data set.

%TODO(maybe cite paper showing that segmentation contains most data)

\begin{figure}
	\noindent\makebox[1.1\textwidth]{
		\centering
		\input{tikz/preproc}
	}
	\caption{Preprocessing pipeline visualized for a sample image.}
	\label{fig:preproc}
\end{figure}

Note that since the primary output of our preprocessing pipeline is based entirely on segmentation masks, one could combine the T1-weighted scans with data from different brain imaging modalities such as T2-weighted MRI data or proton density (PD) scans, therefore drastically increasing the number of possible data sources. However, we do not validate or pursue this idea in the context of this thesis.

For computational reasons, we also perform downsampling with a factor of \sfrac{1}{2} followed by cropping to keep only the center 32 coronal slices, resulting in a final shape of $ 80 \times 32 \times 80 $. However, note that our architecture uses 3D convolutions throughout and therefore can be trained on full-size data if desired, albeit at a significant computational penalty.

\subsection{Data Splitting}
\label{sec:datsplitting}
In order to run our experiments, we generate a number of data sets for the different settings. For the sake of notational brevity and conciseness, let $s$ denote one subject in our data set and let $v^s_i$ denote the $i$-th visit of subject s, with $V(s)$ denoting the temporally ordered set of visits $v^s_i \in V(s)$ of subject $s$, for $i \in 1 \ ..\ |V(s)|$. To improve readability, we generally omit $s$ unless required. For redundancy, MRI scans are usually performed twice resulting in two separate but very similar images for the same visit. Moreover, images taken at different magnetic field strength levels are available for some subjects. As a consequence, a visit $v_i$ typically consists of multiple images $x_{ik} \in I(v_i)$ along with the corresponding image meta data. Finally, the examdate $e_i$ and the subject age $a_i$ at time $e_i$ as well as the diagnosis $d_i$ are available for most visits.

Our data sets are divided into five equal splits $ \{ \mathcal{S}_i \}_{i\ \in\ 0\;..\;4} $ of 20\% each which are used in various different configurations detailed in the corresponding experiment's section. We perform this split on a subject basis and do so globally, in the sense that if a subject appears in a specific data set, it is always assigned to the same split. In other words, for any pair of splits $ \mathcal{S}_i^A $ and $ \mathcal{S}_j^B $ taken from data sets $A$ and $B$, with $ i \neq j $ (for instance $\mathcal{S}^{\scaleto{Base}{5pt}}_0$ and $\mathcal{S}^{\scaleto{Pairs}{5pt}}_1$), the intersection $ { \mathcal{S}_i^A \cap \mathcal{S}_j^B = \varnothing } $ is guaranteed to be empty. In addition to these five splits, we keep a separate $\mathcal{S}_{conv}$ containing all images of subjects which are contained in the MCI conversion data set explained below.

\subsubsection*{Base Image Set} \label{sec:datsingles}
The \textit{Base Image Set} forms the foundation for all other data sets. It consists of all images $x_{ik}$ and the meta data for the corresponding visits $v_i$ for which $e_i$ and $a_i$ are obtainable. The primary use for this set is in the training of our age regressor models.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{c c c c c c c c c c c}
			\toprule
			\multirow{2}{*}{\bfseries Split} & 
			\multirow{2}{*}{\bfseries N} & 
			\multirow{2}{*}{\bfseries S} & 
			\multicolumn{2}{c}{\bfseries HC} & 
			\multicolumn{2}{c}{\bfseries MCI} & 
			\multicolumn{2}{c}{\bfseries AD} &
			\multicolumn{2}{c}{\bfseries Age} \\
			\cmidrule(lr){4-5}
			\cmidrule(lr){6-7}
			\cmidrule(lr){8-9}
			\cmidrule(lr){10-11} 
			& & & N & S & N & S & N & S & $\mu$ & $\sigma$ \\ 
			%------
			\cmidrule(lr){1-11}
			$\mathcal{S}_0$      &  2944 &  503 & 1286 &  274 &  961 &  172 &  697 & 137 & 75.1 & 7.4 \\
			$\mathcal{S}_1$      &  2905 &  504 & 1198 &  266 & 1018 &  181 &  689 & 139 & 75.6 & 7.2 \\
			$\mathcal{S}_2$      &  3202 &  506 & 1435 &  269 & 1036 &  178 &  731 & 132 & 76.3 & 7.4 \\
			$\mathcal{S}_3$      &  3294 &  504 & 1563 &  256 & 1025 &  192 &  706 & 136 & 75.6 & 7.4 \\
			$\mathcal{S}_4$      &  2947 &  506 & 1242 &  264 &  992 &  189 &  713 & 147 & 75.1 & 7.6 \\
			$\mathcal{S}_{conv}$ &  4188 &  271 &  174 &   14 & 3184 &  271 &  830 &  98 & 75.2 & 7.1 \\
			\cmidrule(lr){1-11}
			All             & 19480 & 2794 & 6898 & 1343 & 8216 & 1183 & 4366 & 789 & 75.5 & 7.4 \\
			\bottomrule
		\end{tabular}
		\caption{Overview of the base image set. $N$ refers to the number of separate images and $S$ to the number of distinct subjects. Note that for any split, the sum of subjects over all diagnoses generally exceeds the total number of subjects, since one subject may have images with different diagnoses.}
		\label{tab:baseset}
	\end{center}
\end{table}

\subsubsection*{MCI/AD Set} \label{sec:datmciad}
The \textit{MCI/AD Set} consists of the images of all visits $v_i$ for which the diagnosis $d_i \in \{MCI, AD\}$ and we have high confidence in the label, defined as follows:

We consider a visit $v_i$ \textit{firmly MCI} if $d_i$ as well as both the diagnoses of the previous and following visit $d_{i-1}$ and $d_{i+1}$ are $MCI$. Implicitly, this also means that we only consider subjects with at least three visits.

Conversely, for a visit $v^s_i$ to be considered \textit{firmly AD}, we require that both $d^s_i$ and $d^s_{i-1}$, the current and previous diagnoses, are $AD$.

Note that following these definitions, it is possible for one subject to have visits in both the MCI \textit{and} AD group, see \autoref{fig:mciad} for an illustrated example. An overview of the data set is shown in \autoref{tab:mciadset}.

\begin{figure}[h]
	\centering
	\input{tikz/mciad}
	\caption{Illustation of MCI and AD visits, \Large$\circ$\normalsize\;= MCI, $\times$ = AD}
	\label{fig:mciad}
\end{figure}

\vspace{20pt}

\begin{table}[h]
	\begin{center}
		\begin{tabular}{c c c c c c c c c c c}
			\toprule
			\multirow{2}{*}{\bfseries Split} & 
			\multirow{2}{*}{\bfseries N} & 
			\multirow{2}{*}{\bfseries S} & 
			\multicolumn{2}{c}{\bfseries MCI} & 
			\multicolumn{2}{c}{\bfseries AD} & 
			\multicolumn{2}{c}{\bfseries MCI $\cap$ AD} &
			\multicolumn{2}{c}{\bfseries Age} \\
			\cmidrule(lr){4-5}
			\cmidrule(lr){6-7}
			\cmidrule(lr){8-9}
			\cmidrule(lr){10-11} 
			& & & N & S & N & S & N & S & mean & std \\ 
			%------
			\cmidrule(lr){1-11}
			$\mathcal{S}_0$ &  787 & 148 &  391 &  82 &  396 &  71 &  40 &  5 & 74.4 & 7.6 \\
			$\mathcal{S}_1$ &  814 & 148 &  408 &  85 &  406 &  71 &  71 &  8 & 76.4 & 7.9 \\
			$\mathcal{S}_2$ &  909 & 150 &  451 &  82 &  458 &  72 &  45 &  4 & 77.0 & 7.9 \\
			$\mathcal{S}_3$ &  845 & 148 &  426 &  83 &  419 &  71 &  75 &  6 & 76.0 & 7.9 \\
			$\mathcal{S}_4$ &  767 & 150 &  372 &  89 &  395 &  72 &  90 & 11 & 74.9 & 8.0 \\
			\cmidrule(lr){1-11}
			All             & 4122 & 744 & 2048 & 421 & 2047 & 357 & 321 & 34 & 75.8 & 7.9 \\
			\bottomrule
		\end{tabular}
		\caption{Overview of the MCI/AD data set. $N$ refers to the number of separate images and $S$ to the number of distinct subjects. For MCI $\cap$ AD, $N$ refers to the number of images from subjects for which we have images in both groups.}
		\label{tab:mciadset}
	\end{center}
\end{table}

\subsubsection*{Image Pairs Set}
\label{sec:datpairs}
The \textit{Image Pairs Set} consists of pairs of images $(x_i, x_j)$ of subject $s$ at two different visits $v_i$ and $v_j$. Of particular importance is the time step $t_{ij} = e_j - e_i$ between $v_i$ and $v_j$. We limit the maximum time step to 6 years for computational reasons explained in \autoref{sec:adaarbtimestep}. As visualized in \autoref{fig:timestephist}, the data set is biased towards smaller time steps with median of $1.53$, mean of $2.00$ and a standard deviation of $1.47$ years. To mitigate this, we calculate sample weights $w = {(| t - \bar t | + 1)}^{\sfrac{1}{2}}$, where $\bar t$ is the mean over all time steps $t$. The resulting distribution is shown in \autoref{fig:timestephist}. We also visualize the $L_1$ difference between $x$ and $y$ for all pairs in \autoref{fig:l1plots}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{l c c | c c c c c c}
			\toprule
			& 
			\multicolumn{2}{c}{All} &
			\multicolumn{2}{c}{\bfseries HC} & 
			\multicolumn{2}{c}{\bfseries MCI} & 
			\multicolumn{2}{c}{\bfseries AD} \\
			\cmidrule(lr){2-3}
			\cmidrule(lr){4-5}
			\cmidrule(lr){6-7}
			\cmidrule(lr){8-9}
			& N & \multicolumn{1}{c}{S} & N & S & N & S & N & S \\ 
			\cmidrule(lr){1-9}
			%------
			\bfseries HC  &  7339 &  674 & 6739 & 649 &  495 & 110 &  105 &  27 \\
			\bfseries MCI &  5399 &  918 &  469 &  83 & 3698 & 737 & 1232 & 331 \\
			\bfseries AD  &  2019 &  421 &    1 &   1 &   43 &  18 & 1975 & 416 \\
			\cmidrule(lr){1-9}
			All           & 14757 & 1789 & 7209 & 673 & 4236 & 794 & 3312 & 646 \\
			\bottomrule
		\end{tabular}
		\caption{Overview of the image pairs set, showing the number of pairs for all combinations of diagnoses. Rows correspond to the first image, columns to the second. $N$ refers to the number of separate images and $S$ to the number of distinct subjects.}
		\label{tab:pairsset}
	\end{center}
\end{table}

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\linewidth, trim={10 30 30 30}, clip]{images/l1_diff_plots/brains} 
		\caption{full-size}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\linewidth, trim={10 30 30 30}, clip]{images/l1_diff_plots/slice.pdf}
		\caption{cropped}
	\end{subfigure}
	 
	\caption{$L_1$ difference between image pairs $(x, y)$ from our data set for both the full-size and the cropped scans. We observe that while the difference increases with $t$, it does so rather slowly with slopes of 0.007 and 0.011 respectively.}
	\label{fig:l1plots}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{images/timestep_plots/hist} 
	\caption{The histogram for time step $t$ across all pairs in the pairs data set (in purple). To mitigate the bias towards smaller $t$, we calculate sample weights to be used during training of our generator models and show the distribution after applying the weights (in cyan).}
	\label{fig:timestephist}
\end{figure}


\subsubsection*{MCI Conversion Set} \label{sec:datconv}
The \textit{MCI Conversion Set} consists of image pairs $(x_i, x_j)$ of progressive and stable MCI subjects according to the definitions in \autoref{sec:appconvpred}. Adding to these constraints, for a subject to be considered pMCI we further require a minimum of two visits diagnosed as MCI and AD each. Furthermore, the subject's diagnosis may not revert from AD to MCI at any point in time.

Following the notation in \ref{sec:appconvpred}, we choose the time step between $v_i$ and $v_j$ to be $\Delta = 4$ based on the available data as well as previous work in \cite{wegmayr2019generative}. In general, multiple viable image pair combinations exist for each subject. We prioritize matching $\Delta$ followed by centering the point in time where the diagnosis change occurs within $\Delta$. \autoref{fig:pmci} shows one such pair of visits for a pMCI subject. In total, the conversion set contains 271 pairs of images from 271 subjects, of which 98 are pMCI and 173 are sMCI.

\begin{figure}[h]
	\centering
	\input{tikz/pmci}
	\caption{Illustration of a pMCI image pair, \Large$\circ$\normalsize\;= MCI, $\times$ = AD}
	\label{fig:pmci}
\end{figure}

Note that due to its use in the model validation, this data set represents a separate independent split $\mathcal{S}_{conv}$, that is, subjects which are part of the MCI Conversion Set do not occur in any other split.
\begin{comment}
\subsubsection*{Leave One Visit Out} \label{sec:datloo}
Finally, as the name implies, the \textit{Leave One Visit Out Set} is formed by assigning one visit $v^s_i \in V(s)$ at random to the validation set for all subjects $s$ with $ | V(s) | \ge 3 $. All remaining visits are placed in the training set. Note that due to these criteria, this data set does not follow the global 5-fold split. See \autoref{tab:looset} for an overview of the data set.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{c c c c c c c c c c c}
			\toprule
			\multirow{2}{*}{\bfseries Split} & 
			\multirow{2}{*}{\bfseries N} & 
			\multirow{2}{*}{\bfseries S} & 
			\multicolumn{2}{c}{\bfseries HC} & 
			\multicolumn{2}{c}{\bfseries MCI} & 
			\multicolumn{2}{c}{\bfseries AD} &
			\multicolumn{2}{c}{\bfseries Age} \\
			\cmidrule(lr){4-5}
			\cmidrule(lr){6-7}
			\cmidrule(lr){8-9}
			\cmidrule(lr){10-11} 
			& & & N & S & N & S & N & S & mean & std \\ 
			%------
			\cmidrule(lr){1-11}
			$\mathcal{S}_{train}$ & 16240 & 2794 & 5867 & 1338 & 6834 & 1164 & 3539 & 767 & 75.5 & 7.4 \\
			$\mathcal{S}_{valid}$ &  1599 & 1599 &  544 &  544 &  666 &  666 &  389 & 389 & 75.3 & 7.3 \\
			\cmidrule(lr){1-11}
			All                   & 17839 & 2794 & 6411 & 1343 & 7500 & 1183 & 3928 & 789 & 75.5 & 7.4 \\
			\bottomrule
		\end{tabular}
		\caption{Overview of the Leave One Visit Out data set. $N$ refers to the number of separate images and $S$ to the number of distinct subjects.}
		\label{tab:looset}
	\end{center}
\end{table}

\end{comment}


\section{Age Regressor}
\label{sec:expreg}
Validating the performance of a generative model is a hard problem in general. Beyond visual inspection of the outputs, we also obtain an estimation of the age label, and by extension the time step, by applying a pre-trained age regressor to our model's outputs. Furthermore, the regressor is also included in the generator loss function as described in \autoref{sec:adaagereg}. Given its importance in the validation of our generative model, we evaluate the regressor's performance on a number of different tasks.

The regressor is implemented as a 3D CNN with nine layers of which eight use batch normalization, and trained using the Adam optimizer with $\alpha = 0.001, \beta_1 = 0.9, \beta_2 = 0.999 $ and $ \epsilon = 0.0001 $.

\begin{figure}[h]
	\centering
	\input{tikz/arch_reg}
	\caption{Network architecture of the age regressor.}
	\label{fig:archreg}
\end{figure}

\subsection*{Absolute Error}
First, we train the regressor on 50'000 batches of 32 samples each optimizing the absolute mean error as its objective function. Using our 5-fold data split, we perform cross validation which yields a mean loss of 3.86 years with a standard deviation of 3.05 (see \autoref{tab:expregcrossval}). \autoref{fig:expregxrxrhat} shows the estimated age label $ \hat a_x $ against the ground truth label $a_x$ for one validation split. We note that the estimator tends to the mean, that is, its estimates are most accurate around the data set's mean age of 75 years with a tendency to be low for subjects above the mean and high for subjects below the mean. Given the linear nature of the loss function, this is to be expected.

However, in our generative model, rather than estimating the absolute age of an image, the age regressor is used to estimate the relative age difference $a_y - a_x$ for an image pair $(x, y)$. As disucussed in \autoref{sec:adaagereg}, we expect errors to partially cancel out in this setting and thus evaluate the regressor's performance on our data set of image pairs to confirm this hypothesis. As before, we perform 5-fold cross validation resulting in a mean of 1.32 and a standard deviation of 1.61, and report the detailed results in \autoref{tab:expregcrossval}. \autoref{fig:expregxryr} compares the absolute losses for $x$ and $y$ and shows the error cancelling effect. Furthermore, we also estimate the age difference $\hat a_y - \hat a_x$ for one split and visualize it against the ground truth time step in \autoref{fig:expregddhat}. Using linear regression on all data points, we obtain an intercept of 0.13 and a slope of 0.61, indicating that while the age regressor is capable of distinguishing different time steps, its estimates tend to be lower than the ground truth time step. This can be explained by closer examination of \autoref{fig:expregxrxrhat} which demonstrates that since the estimate $\hat a_x$ tends to the mean for labels further away from it, relative age differences are subject to a shrinking effect. 

Note that since we are performing these experiments on real image pairs, the results represent an upper bound for the performance of our generative model.

\begin{figure}
	\centering
	\includegraphics[width=.85\linewidth]{images/age_plots/xr_xrhat} 
	\vspace*{-15pt}
	\caption{Age regressor estimates on 2937 validation samples from our base image data set. Note how the predicitions tend to the mean, with a slope of 0.58 compared to the target of 1 indicated in black. The subject's diagnosis does not appear to have a significant impact on the performance. Indicated in red, we observe that as we move away from the mean, the predicted relative time differences between two images shrink as a consequence of the estimator's tendency to the mean.}
	\label{fig:expregxrxrhat}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=.8\linewidth]{images/age_plots/xr_yr} 
	\vspace*{-15pt}
	\caption{Visualization of the error cancelling effect on 2651 validation image pairs from our pairs data set. Each axis represents the difference between the predicted and true age labels for one image. We observe that while the absolute error for a prediction on a single image is quite significant with a mean of 3.58, the estimate of the relative age difference between two images from the same subject is considerably more accurate with a mean error of 1.21. The brightness of each data point corresponds to the averaged true age labels of the image pair, revealing that the loss is correlated with the age of the patient.}
	\label{fig:expregxryr}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\linewidth, trim={18 30 30 30}, clip]{images/age_plots/d_dhat_real} 
		\vspace{-20pt}
		\caption{diagnoses}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\linewidth, trim={18 30 30 30}, clip]{images/age_plots/d_dhat_rolling_real} 
		\vspace{-20pt}
		\caption{mean and standard deviation}
	\end{subfigure}
	 
	\caption{Age regressor estimates on 2651 validation samples from our pairs data set, comparing the estimated age difference to the ground truth time step. Linear regression yields an intercept of 0.13 and a slope of 0.61. Note that the stripe patterns forming along the $x$-axis are a consequence of the study scheduling which mandates follow-up visits in intervals of six or twelve months.}
	\label{fig:expregddhat}
\end{figure}

\begin{table}[h]
	\begin{center}
		\begin{tabular}{c c c c c | c c c c}
			\toprule
			 & 
			\multicolumn{4}{c}{absolute age} & 
			\multicolumn{4}{c}{age difference} \\
			\cmidrule(lr){2-5}
			\cmidrule(lr){6-9}
			\multirow{2}{*}{Split} &
			\multicolumn{2}{c}{absolute} & 
			\multicolumn{2}{c}{squared} &
			\multicolumn{2}{c}{absolute} & 
			\multicolumn{2}{c}{squared} \\
			\cmidrule(lr){2-3}
			\cmidrule(lr){4-5}
			\cmidrule(lr){6-7}
			\cmidrule(lr){8-9}
			 & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\ 
			%------
			\cmidrule(lr){1-9}
			$\mathcal{S}_0$ & 3.58 & 2.92 & 3.62 & 2.96 & 1.37 & 1.65 & 1.32 & 1.53 \\
			$\mathcal{S}_1$ & 3.69 & 2.93 & 3.87 & 2.97 & 1.20 & 1.55 & 1.09 & 1.55 \\
			$\mathcal{S}_2$ & 4.07 & 3.01 & 4.09 & 2.97 & 1.30 & 1.58 & 1.29 & 1.51 \\
			$\mathcal{S}_3$ & 4.07 & 3.07 & 3.89 & 2.90 & 1.36 & 1.55 & 1.33 & 1.60 \\
			$\mathcal{S}_4$ & 3.90 & 3.31 & 4.06 & 3.32 & 1.37 & 1.72 & 1.35 & 1.62 \\
			\cmidrule(lr){1-9}
					& 3.86 & 3.05 & 3.90 & 3.02 & 1.32 & 1.61 & 1.28 & 1.56 \\
			\bottomrule
		\end{tabular}
		\caption{Cross validated mean and standard deviation of our age regressor model on the tasks of predicting the age label of a single image as well as estimating the age difference for a pair of images. We also compare models trained using absolute and squared loss objectives.}
		\label{tab:expregcrossval}
	\end{center}
\end{table}

\subsection*{Squared Error}
We also examine the performance of the same architecture optimizing the mean squared error, with very similar results presented in \autoref{tab:expregcrossval}

\begin{comment}
\subsection*{Leave One Visit Out}
Given that the age regressor is exclusively utilized to estimate relative age differences in our generative model, we perform an additional experiment more directly focussed at that metric. To that end, we train the same model architecture for 100'000 batches of 32 images each from the Leave One Visit Out data set outlined in \autoref{sec:datloo}. We obtain a mean absolute validation error of 1.01 years which, while significanlty lower than the mean absolute error on a single image, is still relatively large considering the simplified problem setting. However, the value is similar to the loss in the pair time step setting as expected.
\end{comment}

\section{Diagnosis Classifier}
\label{sec:expdxclf}
We pre-train a diagnosis classifier to discriminate between images labeled as MCI and AD respectively. Given the gradual transition between the two diagnoses, we use the \textit{MCI/AD Set} described in \autoref{sec:datmciad}, which limits our training and validation sets to a subset of images we consider \textit{firmly MCI} or \textit{firmly AD}. Note that we exclude all subjects in the healthy control group HC and use a binary classifier focussing on the more subtle distinctions between the effects of MCI and AD.

The classifier is implemented as a 3D CNN identical in structure to the age regressor. Softmax cross entropy is used as the objective function and minimized using the Adam optimizer with $\alpha = 0.0001, \beta_1 = 0.9, \beta_2 = 0.999$ and $\epsilon = 0.01$ for increased training stability.
We perform cross validation using the 5-fold data split, yielding an accuracy of 70.10\% and an $\text{F}_1$-score of 70.60\%. Due to high variance in the classifier's accuracy, we train the model five times for every fold in order to avoid suboptimal local minima and select the run with the highest accuracy. Each model is trained on 10'000 batches of 32 samples each and we report the detailed results in \autoref{tab:clfcrossval}.

\begin{table}[h]
	\begin{center}
		\makebox[\textwidth][c]{
		\begin{tabular}{c c c c c c c c c c c}
			\toprule
			\multirow{2}{*}{Split} & 
			\multicolumn{2}{c}{Run 1} & 
			\multicolumn{2}{c}{2} & 
			\multicolumn{2}{c}{3} & 
			\multicolumn{2}{c}{4} & 
			\multicolumn{2}{c}{5} \\
			\cmidrule(lr){2-3}
			\cmidrule(lr){4-5}
			\cmidrule(lr){6-7}
			\cmidrule(lr){8-9}
			\cmidrule(lr){10-11}
			 & acc & $\text{F}_1$ & acc & $\text{F}_1$ & acc & $\text{F}_1$ & acc & $\text{F}_1$ & acc & $\text{F}_1$ \\ 
			%------
			\cmidrule(lr){1-11}
			$\mathcal{S}_0$ & 68.58 & 66.76 & 69.08 & 66.76 & \textbf{70.74} & \textbf{69.50} & 67.81 & 65.39 & 63.74 & 63.51 \\
			$\mathcal{S}_1$ & \textbf{70.32} & \textbf{72.07} & 66.13 & 68.13 & 68.10 & 70.26 & 69.95 & 72.01 & 70.07 & 71.84 \\
			$\mathcal{S}_2$ & 67.33 & 70.24 & 67.56 & 69.58 & 65.33 & 67.97 & \textbf{67.78} & \textbf{69.79} & 64.89 & 67.76 \\
			$\mathcal{S}_3$ & \textbf{71.68} & \textbf{71.03} & 70.14 & 69.86 & 70.26 & 69.94 & 68.13 & 67.31 & 70.50 & 68.52 \\
			$\mathcal{S}_4$ & 66.71 & 67.60 & 66.58 & 67.59 & \textbf{69.97} & \textbf{70.59} & 68.15 & 68.56 & 64.88 & 64.28 \\
			%\cmidrule(lr){1-11}
			%                & 3.86 & 3.05 & 3.90 & 3.02 & & & & & & \\
			\bottomrule
		\end{tabular}
}
		\caption{Cross validated accuracy and $\text{F}_1$-score of our classifier models.}
		\label{tab:clfcrossval}
	\end{center}
\end{table}

\section{Diffeomorphic Models}
\begin{comment}
\subsection{Voxelmorph}
To establish a baseline, we first train a variation of the Voxelmorph architecture described in \autoref{sec:vox} with minimal modifications to support the brain aging task. These modifications include removing the target image $y$ from the generator input as well as extending the scaling and squaring method to produce output deformations $\Phi^{(t)}$ for arbitrary $t$ as described in \autoref{sec:adaarbtimestep}. Regarding hyperparameters, we use $\lambda = 25$ as suggested in \cite{dalca2018unsupervised} and reduce $\sigma$ to $0.01$ to account for the smaller magnitude of the changes in the brain aging setting discussed in \autoref{sec:ada}.
\end{comment}
\subsection{Synthetic Data}
To validate our modifications to the Voxelmorph architecture, we first train the model on 1'000 batches of 4 samples each from the synthetic data set described in \autoref{sec:datsynth}. Visual inspection of the results, presented in \autoref{fig:toydata} confirms the model's ability to learn, and integrate over, a stationary velocity field to generate deformation fields for variable time steps $t$. Furthermore, we demonstrate the effectiveness of the sparseness penalty in suppressing the magnitude of the deformation field in areas of little change. Note that neither the age regressor nor the diagnosis classifier are used in any of these experiments as there are no corresponding features in the synthetic data samples.

\begin{figure}
	%TODO(maybe flip time bar)
	\centering
	\input{tikz/toydata}
	\vspace*{-15pt}
	\caption{Model outputs generated using the synthetic data set. We train the model three times, on noisy or solid backgrounds, with or without applying a sparseness penalty. Each row represents one sample output consisting of the original, the target, and the generated image, the time step $t$ scaled to $[0, 1)$, the velocity field $v$ and its magnitude, as well as its separate dimensions. Note how the sparseness penalty leads to a more interpretable deformation field.}
	\label{fig:toydata}
\end{figure}

\subsection{MRI Data}

We train the brain aging model in a number of different configurations and on different data splits, as listed in \autoref{tab:ganconfig}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{c c c c c c c}
			\toprule
			\# & $\lambda_{age}$ & $\lambda_{dx}$ &
				$\mathcal{S}_{train}$ & $\mathcal{S}_{valid}$ & $\mathcal{S}_{conv}$ & Pairs \\
			%------
			\cmidrule(lr){1-7}
			1 &  -  &  -  & $\mathcal{S}_1\:\mathcal{S}_2\:\mathcal{S}_3\:\mathcal{S}_4$ &
				      $\mathcal{S}_0$ &
				      $\mathcal{S}_{conv}$ &
				      \footnotesize any $\rightarrow$ any \\
			2 & 100 &  -  & $\mathcal{S}_1\:\mathcal{S}_3$ & 
				      $\mathcal{S}_2\:\mathcal{S}_4$ &
				      $\mathcal{S}_0$ & 
				      \footnotesize any $\rightarrow$ any \\
			3 &  -  & 100 & $\mathcal{S}_1\:\mathcal{S}_3$ & 
				      $\mathcal{S}_2\:\mathcal{S}_4$ &
				      $\mathcal{S}_{conv}$ & 
				      \footnotesize any $\rightarrow$ MCI/AD \\
			4 & 100 & 100 & $\mathcal{S}_1\:\mathcal{S}_3$ & 
				      $\mathcal{S}_2\:\mathcal{S}_4$ &
				      $\mathcal{S}_0\:\mathcal{S}_{conv}$ &
				      \footnotesize any $\rightarrow$ MCI/AD \\
			5 &  -  &  -  & $\mathcal{S}_1\:\mathcal{S}_2\:\mathcal{S}_3\:\mathcal{S}_4$ &
				      $\mathcal{S}_0$ &
				      - &
				      \footnotesize MCI/AD $\rightarrow$ AD \\
			6 &  -  &  -  & $\mathcal{S}_1\:\mathcal{S}_2\:\mathcal{S}_3\:\mathcal{S}_4$ &
				      $\mathcal{S}_0$ &
				      - &
				      \footnotesize HC $\rightarrow$ HC \\
			\bottomrule
		\end{tabular}
		\caption{Overview of the generator model configurations.}
		\label{tab:ganconfig}
	\end{center}
\end{table}

For the remaining hyperparameters, we use $\lambda_{kl} = 10$, $\lambda_{prior} = 25$, $\lambda_{sim} = 200$ and $\lambda_{sparse} = 0$ due to negative performance impacts even for small sparseness loss weights. During training, the velocity field $v$ is integrated using up to $T = 7$ squaring layers, resulting in a maximum number of $128$ atomic steps. In combination with the maximum time step of 6 years in our pairs data set, this corresponds to a temporal precision of approximately 17 days.

Note that the regressor and the classifier used as loss terms in configurations 2, 3, and 4 are trained on $\mathcal{S}_{valid}$ and evaluated on $\mathcal{S}_{train}$ in order to prevent them from overfitting on the generator training data.

The model is trained using the Adam optimizer with $\alpha = 0.0001$, $\beta_1 = 0.0$, $\beta_2 = 0.9$ and $\epsilon = 10^{-7}$. Limited by GPU memory, we use batch size of 8 and train the model until convergence, in the the case of configurations 1, 5 and 6, and until $\mathcal{L}_{age}$ or $\mathcal{L}_{dx}$ display signs of overfitting for configurations 2, 3 and 4. Following the training procedure in \cite{goodfellow2014generative}, we alternate between training the critic and the generator on five and one batches respectively. Each such training step runs in roughly 5 seconds per batch or 0.6 seconds per sample. We present three sample generator outputs in \autoref{fig:expsamples}.

\begin{figure}
	%TODO(maybe flip time bar)
	\centering
	\input{tikz/mridata}
	\vspace*{-15pt}
	\caption{Model outputs generated using configuration 1 on validation image pairs. Each row represents one sample output consisting of the original, the target, and the generated image, the time step $t$ scaled to $[0, 1)$, the velocity field $v$ and its magnitude, as well as its separate dimensions..}
	\label{fig:expsamples}
\end{figure}

\subsubsection*{Follow-Up Prediction}
As our first experiment, we predict follow-up images using the actual time steps in the pairs data set. Using the pre-trained age regressor, we evaluate the performance on this task for configurations 1 and 2 and present the results in \autoref{fig:expfollowuprolling}. Both model configurations produce outputs that appear aged to the regressor, with mean absolute errors between the predicted and the actual time step of 1.28 and 1.20 years respectively. We further observe that configuration 2 displays a smaller rolling standard deviation in \autoref{fig:expfollowuprolling}, indicating higher confidence in its outputs.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\linewidth, trim={30 30 30 30}, clip]{images/age_plots/d_dhat_rolling_config1} 
		\caption{\ Configuration 1}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\linewidth, trim={30 30 30 30}, clip]{images/age_plots/d_dhat_rolling_config2} 
		\caption{\ Configuration 2}
	\end{subfigure}
	 
	\caption{Rolling mean and standard deviation of the age labels predicted by the age regressor on generated images. We compare configurations 1 and 2, and observe a decrease in the standard deviation for configuration 2 which uses an age regressor as part of its loss function. For comparison, \autoref{fig:expregddhat} shows the results of the same experiment performed on real image pairs.}
	\label{fig:expfollowuprolling}
\end{figure}

\subsubsection*{Fixed Time Step Prediction}
To evaluate the model's ability to generate follow-up images at fixed time steps, we predict images at $t \in \{1, 2, 4, 6, 8\}$ years and estimate the corresponding age labels using the age regressor. The results for configurations 1 and 2 are presented in \autoref{tab:expfixed} and visualized in \autoref{fig:expfixedhist}. We observe that the time steps estimated on the generated images tend to be higher than the target time step $t$ for small $t$.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{c c c c c c c c c c c}
			\toprule
			\multirow{2}{*}{\#} &
			\multicolumn{2}{c}{$t = 1$} &
			\multicolumn{2}{c}{2} &
			\multicolumn{2}{c}{4} &
			\multicolumn{2}{c}{6} &
			\multicolumn{2}{c}{8} \\
			\cmidrule(lr){2-3}
			\cmidrule(lr){4-5}
			\cmidrule(lr){6-7}
			\cmidrule(lr){8-9}
			\cmidrule(lr){10-11}
			 & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\
			%------
			\cmidrule(lr){1-11}
			1 & 1.90 & 0.95 & 2.60 & 1.36 & 3.30 & 2.24 & 3.51 & 3.01 & 3.35 & 3.69 \\
			2 & 2.08 & 0.79 & 3.02 & 1.10 & 4.01 & 1.74 & 4.66 & 2.30 & 5.02 & 2.78 \\
			\bottomrule
		\end{tabular}
		\caption{Mean and standard deviation of the estimated age labels on outputs generated for various different target time steps $t$.}
		\label{tab:expfixed}
	\end{center}
\end{table}

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.9\textwidth}
		\includegraphics[width=\linewidth, trim={0 20 0 20}, clip]{images/age_plots/hist_fixed_config1} 
		\caption{\ Configuration 1}
	\end{subfigure}
	\begin{subfigure}{0.9\textwidth}
		\includegraphics[width=\linewidth, trim={0 20 0 20}, clip]{images/age_plots/hist_fixed_config2}
		\caption{\ Configuration 2}
	\end{subfigure}
	 
	\caption{Results of the fixed time step experiments for configurations 1 and 2. We generate follow-up images for time steps 1, 2, 4 and 6 on our test data and use the regressor to predict the age labels. Note that the standard deviation increases significantly for larger steps but does so less strongly for configuration 2 which uses an age regressor in its loss function.}
	\label{fig:expfixedhist}
\end{figure}

\subsubsection*{Feature Attribution}
To visualize the differences in aging between healthy subjects and subjects affected by Alzheimer's Disease, we train configurations 5 and 6 on data sets which are accordingly limited to healthy and non-healthy subjects respectively. We qualitatively and quantitatively observe higher mean deformation magnitudes for configuration 6 and visually compare the effects of both models applied to a healthy subject in \autoref{fig:expfeathc} and a subject affected by AD in \autoref{fig:expfeatad}.

\begin{figure}[h]
	\centering
	\input{tikz/feat_ad}
	\vspace{-15pt}
	\caption{The center column consists of a series of real images from a \textbf{subject diagnosed with Alzheimer's Disease}, taken at one-year intervals steps, top to bottom. The two columns to the left show the predicted images at the same time steps using a model trained exclusively on healty patients, as well as the difference maps with respect to the base image. Similarly, the two columns to the right show the predicted images using a model exclusively trained on patients affected by Alzheimer's Disease.}
	\label{fig:expfeatad}
\end{figure}

\begin{figure}[h]
	\centering
	\input{tikz/feat_hc}
	\vspace{-15pt}
	\caption{The center column consists of a series of real images from a \textbf{healthy subject}, taken at one-year intervals steps, top to bottom. The two columns to the left show the predicted images at the same time steps using a model trained exclusively on healty patients, as well as the difference maps with respect to the base image. Similarly, the two columns to the right show the predicted images using a model exclusively trained on patients affected by Alzheimer's Disease.}
	\label{fig:expfeathc}
\end{figure}

% pat_id:	n_imgs	field		remark	base_img

% AD:
% ADNI_1066	7	1.5	pMCI	bad	ADNI_85356
% ADNI_922	7	1.5	pMCI		ADNI_77852
% ADNI_1427	7	1.5	pMCI 		ADNI_153734
% ADNI_42	8	1.5	pMCI	ugly
% ADNI_214	7	1.5	pMCI		ADNI_137475	maybe
% ADNI_906	6	1.5	pMCI	nice	ADNI_120065	yes, for now
% ADNI_887	6	1.5	pMCI		ADNI_120099	really nice, but need better model
% ADNI_658	6	1.5	pMCI		ADNI_62841	nope
% ADNI_752	6	1.5	pMCI		ADNI_66508	almost no changes
% ADNI_331	6	1.5	pMCI		ADNI_141445	yes! but better model
% ADNI_1243	6	1.5	pMCI	nice	ADNI_214258	small changes, okay

% HC:
% ADNI_382	8	1.5			ADNI_107542	okay...
% ADNI_441	8	1.5			ADNI_104027	nope
% ADNI_553	9	1.5			ADNI_155040	real kinda okay, AD model bad
% ADNI_677	8	1.5			ADNI_116851	no, messed up real
% ADNI_303	6	1.5		ugly	
% ADNI_337	10	1.5			ADNI_142666	no, artifacts
% ADNI_298	9	1.5			ADNI_141972	real okay, small ventricles, maybe slice 20, but guess not
% ADNI_734	8	1.5			ADNI_116000	yes, for now, not great though
% ADNI_120	10	1.5			ADNI_137014	good, slice 15
% ADNI_260	9	1.5			ADNI_172291	not really, skull, workable if need be

\subsubsection*{Long-Term Prediction}
\label{sec:explongterm}
As mentioned in \autoref{sec:applongterm}, our model architecture allows generating images for very large time steps, such as 50 years, at reasonable computational cost. However, while the velocity field predicted by the generator is time-invariant in theory, this property is only enforced within the range of time steps occuring in the training data. In practice, using time steps around and beyond the maximum step in the training data very quickly result in unrealistic looking outputs. We use configuration 1 to generate images at time steps of $t \in \{2, 4, 6, 8, 10\}$ years for two samples and present the resulting outputs in \autoref{fig:explongterm}.
Superior results could likely be obtained by iteratively applying deformations for smaller time steps in an approach similar to that used in \cite{wegmayr2019generative}.

\begin{figure}[h]
	\centering
	\input{tikz/longterm}
	\vspace{-15pt}
	\caption{Sequence of a base image $x$ and images generated for time steps of 2, 4, 6, 8 and 10 years, as well as the corresponding difference maps to the base image. We pick two samples which represent common outcomes in our data sets and use configuration 1 to generate the predictions.}
	\label{fig:explongterm}
\end{figure}

\subsubsection*{Conversion Prediction}
As described in \autoref{sec:appconvpred}, we evaluate the performance of our generative model on the MCI conversion prediction task. Given an image pair $(x, y)$ we generate $G(x)$ with time step $t = 4$ using our generative model. We then predict $p_{AD}(G(x))$ using a diagnosis classifier and apply thresholding to classify the subject as progressive or stable.

Based on the results of the Follow-Up and Fixed Time Step Prediction experiments, we calculate adjusted time steps for configurations 1 and 2 according to their aging performance and evalute the conversion probability on images predicted using the adjusted time step. As an example, for $t = 4$, configuration 2 is known to produce images that, to the age regressor, appear aged by 4.01 years. However, for the same $t$, the time step estimated by the age regressor on the real image pairs is significantly lower at a value 2.53 years. Therefore, we rescale $t$ to match the value observed on the real image pairs in order to obtain more realistic predictions.

Keeping in mind the imbalanced nature of the conversion data set, we use balanced accuracy and $\text{F}_1$-score as our metrics

\begin{equation}
	acc = \frac{1}{2}\ \bigg(\frac{TP}{TP + FN} + \frac{TN}{TN + FP}\bigg)
\end{equation}

\begin{equation}
	\begin{split}
		\text{F}_1 =&\ 2\ \bigg(\frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}\bigg)
	\end{split}
\end{equation}
where
\begin{center}
	\vspace{-40pt}
	\parbox{0.7\linewidth}{
		\begin{multicols}{2}
			\begin{equation*}
				\text{precision} = \ \frac{TP}{TP + FP}
			\end{equation*}\break
			\begin{equation*}
				\text{recall} = \ \frac{TP}{TP + FN}
			\end{equation*}
		\end{multicols}
	}
\end{center}

with $TP$ = true positives, $TN$ = true negatives, $FP$ = false positives and $FN$~=~false negatives.

Given a set of images $\{x\}$ from our conversion data set, we independently calculate the balanced conversion accuracy and the $\text{F}_1$-score as follows:
\begin{enumerate}
	\item We use a diagnosis classifier to predict the probabilities $p_{AD}(x)$ for each $x$
	\item We split the conversion data set into 5 balanced splits $\{\mathcal{T}_i\}_{i\ \in\ 0\ ..\ 4}$ and calculate $\tau_i$ for each split as the threshold which maximizes the target metric on $\{\mathcal{T}_j\}_{i \neq j}$.
	\item We calculate the metrics for each split $\mathcal{T}_i$ using the corresponding $\tau_i$ and take the mean over all splits
	\item We repeat steps 2 and 3 five times and take the mean for both metrics
	\item We repeat steps 1 to 4 for each of the five best classifier in \autoref{tab:clfcrossval} and again take the mean
\end{enumerate}

We calculate the balanced accuracy and $\text{F}_1$-score on the set of base images $x$ as a baseline and similarly on the set of target images $y$ as an upper bound. Finally, we calculate the metrics for images generated using configurations 1 to 4 and report the results in \autoref{tab:expconv}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{c c c c c c c c c c}
			\toprule
			\multirow{2}{*}{Config} &
			 &
			\multicolumn{2}{c}{Accuracy} &
			\multicolumn{5}{c}{Classifier} &
			\multirow{2}{*}{$t$} \\
			\cmidrule(lr){3-4}
			\cmidrule(lr){5-9}
			 & & $\mu$ & $\sigma$ & 1 & 2 & 3 & 4 & 5 \\
			%------
			\cmidrule(lr){1-10}
			$x$\hphantom{*} &        & 63.81 & 1.67 & 64.17 & 65.41 & 64.18 & 61.06 & 64.25 &   \\
			$y$\hphantom{*} & +12.09 & 75.90 & 0.90 & 74.52 & 76.17 & 76.69 & 75.80 & 76.31 &   \\
			\cmidrule(lr){1-10}
			1\hphantom{*}   & +2.39  & 66.20 & 1.52 & 65.52 & 65.94 & 65.57 & 65.60 & 68.38 & 4 \\
			1*              & +2.95  & 66.75 & 2.33 & 69.76 & 65.83 & 69.02 & 64.59 & 64.58 & 3 \\
			2\hphantom{*}   & +0.17  & 63.98 & 3.84 & 68.40 & 60.62 & 68.53 & 60.15 & 62.22 & 4 \\
			2*              & +2.62  & 66.43 & 3.74 & 67.92 & 71.00 & 68.97 & 61.17 & 63.11 & 2 \\
			3\hphantom{*}   & +0.90  & 64.71 & 2.21 & 65.38 & 67.98 & 62.73 & 63.01 & 64.46 & 4 \\
			4\hphantom{*}   & +0.90  & 64.71 & 2.97 & 63.37 & 66.44 & 68.88 & 63.03 & 61.83 & 4 \\
			\bottomrule
		\end{tabular}
		\caption*{Balanced Accuracy}
		%\label{tab:expconvaccuracy}
	\end{center}
	\begin{center}
		\begin{tabular}{c c c c c c c c c c}
			\toprule
			\multirow{2}{*}{Config} &
			 &
			\multicolumn{2}{c}{$\text{F}_1$-Score} &
			\multicolumn{5}{c}{Classifier} &
			\multirow{2}{*}{$t$} \\
			\cmidrule(lr){3-4}
			\cmidrule(lr){5-9}
			 & & $\mu$ & $\sigma$ & 1 & 2 & 3 & 4 & 5 \\
			%------
			\cmidrule(lr){1-10}
			$x$\hphantom{*} &        & 54.76 & 2.77 & 52.48 & 56.36 & 55.33 & 51.49 & 58.14 &   \\
			$y$\hphantom{*} & +14.61 & 69.37 & 1.14 & 67.77 & 69.71 & 70.26 & 68.93 & 70.20 &   \\
			\cmidrule(lr){1-10}
			1\hphantom{*}   & +3.97  & 58.73 & 2.08 & 57.58 & 59.08 & 56.93 & 58.72 & 61.34 & 4 \\
			1*              & +5.07  & 59.83 & 2.77 & 63.10 & 60.32 & 61.81 & 58.34 & 55.60 & 3 \\
			2\hphantom{*}   & +1.17  & 55.93 & 4.03 & 61.24 & 51.55 & 59.31 & 52.03 & 55.52 & 4 \\
			2*              & +4.42  & 59.18 & 3.75 & 61.22 & 62.82 & 61.72 & 53.04 & 57.09 & 2 \\
			3\hphantom{*}   & +2.39  & 57.15 & 2.87 & 59.50 & 60.74 & 54.22 & 55.80 & 55.50 & 4 \\
			4\hphantom{*}   & +0.93  & 55.69 & 4.81 & 55.51 & 58.24 & 62.04 & 53.76 & 48.86 & 4 \\
			\bottomrule
		\end{tabular}
		\caption*{$\text{F}_1$-Score}
		%\label{tab:expconvf1score}
	\end{center}
	\caption{Results of the MCI conversion prediction experiment. We calculate the results for the real images $x$ and $y$ and use them as our baseline and upper bound respectively. The configurations annotated with a star use adjusted time steps for the image generation.}
	\label{tab:expconv}
\end{table}

For $t = 4$, configuration 1 yields the best results for both metrics with an improvement over the baseline of 2.39\% for the accuracy and 3.97\% for the $\text{F}_1$-score. Using the adjusted time step of $t = 3$ results in a slight improvement of the mean metrics (2.95\% and 5.07\%). For configuration 2, the adjusted time step $t = 2$ sharply increases both metrics (2.62\% and 4.42\%) while also decreasing the standard deviation.

\chapter{Discussion}

Using the age regressor as part of the loss objective has a clear impact on the model's performance, however, the same does not appear to be the case for the diagnosis classifier. While this may be a consequence of the more limited amount of training data available to these configurations, the diagnosis classifier also appears to be very sensitive to both the data splitting as well as its starting conditions. As such, it might not be a suitable taget to be used as part of the objective function. Most probably, this instability is a consequence of the fairly limited number of training samples available to the classifier, which could be addressed by incorporating additional image data from other studies. As touched upon in \autoref{sec:datpreproc}, our use of segmentation-derived images opens up the possibility of using different image modalities such as PD or T2-weighted MRI scans which greatly expands the number of viable data sources.
We also note that downsampling the images may have a significant negative performance impact on the classifier and that therefore, better results could be obtained using architectures operating on full-size and uncropped images.
Beyond more extensive and higher resolution image data sets, additional data modalities such as cognitive test results as well as biomarkers could be incorporated. Furthermore, both the generator and the classifier might benefit from being trained on longitudinal data, that is, data from multiple visits.

Due to the comparatively high number of components in our architecture, attributing variations in the model's performance to changes of its architecture is non-trivial.
In particular, the large number of hyperparameters combined with training durations of up to two days renders systematic parameter tuning computationally infeasible. On the other hand, we also observe that some of the hyperparameters have limited to no effect, suggesting that the architecture could be simplified.

Finally, we address the issue of validating generative outputs by using an age regressor and find this to be a meaningful metric. However, it might be interesting to explore less abstract metrics in the context of brain aging models.

